---
title: "敏感数据"
description: "通过防止模型看到实际密码来安全地处理敏感信息。"
icon: "shield"
---

## 处理敏感数据

在处理密码等敏感信息时,你可以使用 `sensitive_data` 参数来防止模型看到实际值,同时仍允许它在其操作中引用这些值。

以下是如何使用敏感数据的示例:

```python
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from browser_use import Agent

load_dotenv()

# 初始化模型
llm = ChatOpenAI(
    model='gpt-4o',
    temperature=0.0,
)

# 定义敏感数据
# 模型只会看到键(x_name, x_password),但永远不会看到实际值
sensitive_data = {'x_name': 'magnus', 'x_password': '12345678'}

# 在任务描述中使用占位符名称
task = '访问 x.com 并使用 x_name 和 x_password 登录,然后写一篇关于生命意义的帖子'

# 将敏感数据传递给代理
agent = Agent(task=task, llm=llm, sensitive_data=sensitive_data)

async def main():
    await agent.run()

if __name__ == '__main__':
    asyncio.run(main())
```

在这个示例中:
1. 模型只看到 `x_name` 和 `x_password` 作为占位符。
2. 当模型想要使用你的密码时,它输出 x_password - 我们会将其替换为实际值。
3. 当你的密码在当前页面上可见时,我们会在 LLM 输入中替换它 - 这样模型就永远不会在其状态中拥有它。

警告: 视觉模型仍然可以看到页面的图像 - 其中可能显示了敏感数据。

这种方法确保敏感信息保持安全,同时仍允许代理执行需要身份验证的任务。